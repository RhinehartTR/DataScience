# DATA SCIENCE ASSIGNMENT

## BAHA ATAKAN ARIKAN, MI4.0

### SUPERVISED LEARNING

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('C:/Users/ataka/Desktop/Classes/Data Science/Assignment/Human_Resources.csv')

# to check the target variables first
df.head()

df.shape

numCol = df.select_dtypes(include=['number'])
numVar = num_columns.shape[1]
print(f'There are {numVar} numerical variables in the dataset.')

catCol = df.select_dtypes(include=['object'])
catVar = cat_columns.shape[1]
print(f'There are {catVar} categorical variables in the dataset.')

# to see the types and to check the missing variables

df.info()

df.isnull().sum()
sns.heatmap(df.isnull(), cbar=False)
pd.DataFrame((df.isnull().sum() / df.shape[0])*100, columns=['Missing Values Ratio'])

# as we can see, there are no missing values

df['Attrition'].value_counts()

df_raw = df.copy()

# to see which columns are booleans, yes/y or no/n
binary_columns = df.apply(lambda x: x.isin(["Yes", "No", "Y", "N"]).any(), axis=0)
print(binary_columns)

# converting "Yes" to 1 and "No" to 0 for the "Attrition" column
df["Attrition"] = df["Attrition"].map({"Yes": 1, "No": 0})

# converting "Y" to 1 and "N" to 0 for the "Over18" column
df["Over18"] = df["Over18"].map({"Y": 1, "N": 0})

# converting "Yes" to 1 and "No" to 0 for the "OverTime" column
df["OverTime"] = df["OverTime"].map({"Yes": 1, "No": 0})

df.head()

# to check again
print(df[['Attrition', 'Over18', 'OverTime']])

# histograms of the whole dataframe 
for columns in df.columns:
    plt.hist(df[columns])
    plt.title(columns)
    df.hist(figsize=(20,15))
    plt.show()

# to check the correlation and its heatmap

corr = df.corr()

sns.heatmap(corr, annot=True)

plt.show()

print(corr.Attrition)

# dropping the suitable ones

df_code = df.drop(columns=["EmployeeCount", "EmployeeNumber", "Over18", "StandardHours"])

df_code.head()

# SCALING
from sklearn.preprocessing import MinMaxScaler
df_code = pd.get_dummies(df_code)

scaler = MinMaxScaler()
scaler.fit(df_code)
scaled_data = scaler.transform(df_code)
scaled_df = pd.DataFrame(scaled_data, columns=df_code.columns)

Y = scaled_df['Attrition'].values
X = scaled_df.drop(['Attrition'], axis = 1)

valX = X.values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(valX, Y, test_size=0.25, random_state=40)

# MODELLING
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

modelRF = RandomForestRegressor()
modelRF.fit(X_train, y_train)

# MSE check
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_predrf)
print(f"Mean squared error rf: {mse:.6f}")
mse = mean_squared_error(y_test, y_predlr)
print(f"Mean squared error lr: {mse:.6f}")

from sklearn.metrics import classification_report

y_predRF = modelRF.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE: {rmse:.2f}")

modelLR = LinearRegression()
modelLR.fit(X_train, y_train) 
y_predLR = modelLR.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_predLR))
print(f"RMSE: {rmse:.2f}")

### y is continuous -> regression = Linear Regression, Random Forest, KN, SVC etc. 
y is categorical -> classification = Logistic Regression, Random Forest, KNN, SVM etc.
